
Learning to rank[ref]
. Slides from Tie-Yan Liu's talk at WWW 2009 conference are available online
[/ref] or machine-learned ranking (MLR) is a type of supervised or semi-supervised machine learning problem in which the goal is to automatically construct a ranking model from training data. Training data consists of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. "relevant" or "not relevant") for each item. Ranking model's purpose is to rank, i.e. produce a permutation of items in new, unseen lists in a way, which is "similar" to rankings in the training data in some sense.
Learning to rank is a relatively new research area which has emerged in the past decade.

== Applications ==

=== In information retrieval ===

Ranking is a central part of many information retrieval problems, such as document retrieval, collaborative filtering, sentiment analysis, computational advertising (online ad placement).
A possible architecture of a machine-learned search engine is shown in the figure to the right.
Training data consists of queries and documents matching them together with relevance degree of each match. It may be prepared manually by human assessors (or raters, as Google calls them),
who check results for some queries and determine relevance of each result. It is not feasible to check relevance of all documents, and so typically a technique called pooling is used — only top few documents, retrieved by some existing ranking models are checked. 
 Alternatively, training data may be derived automatically by analyzing ''clickthrough logs'' (i.e. search results which got clicks from users),<ref name="Joachims2002">
[/ref] query chains, or such search engines' features as Google's SearchWiki.
Training data is used by a learning algorithm to produce a ranking model which computes relevance of documents for actual queries.
Typically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used. First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as vector space model, boolean model, weighted AND,[ref]
. Section [http://nlp.stanford.edu/IR-book/html/htmledition/efficient-scoring-and-ranking-1.html 7.1]</ref> In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents.

=== In other areas ===

Learning to rank algorithms have been applied in areas other than information retrieval:
* In machine translation for ranking a set of hypothesized translations;
* In computational biology for ranking candidate 3-D structures in protein structure prediction problem.
* In proteomics for the identification of frequent top scoring peptides.
* In Recommender system for identifying a ranked list of related news articles to recommend to a user after he or she has read a current news article.Yuanhua Lv, Taesup Moon, Pranam Kolari, Zhaohui Zheng, Xuanhui Wang, and Yi Chang, Learning to Model Relatedness for News Recommendation, in International Conference on World Wide Web (WWW), 2011.

== Feature vectors ==

For convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called feature vector. Such approach is sometimes called bag of features and is analogous to bag of words and vector space model used in information retrieval for representation of documents.
Components of such vectors are called features, factors or ranking signals. They may be divided into three groups (features from document retrieval are shown as examples):
* Query-independent or static features — those features, which depend only on the document, but not on the query. For example, PageRank or document's length. Such features can be precomputed in off-line mode during indexing. They may be used to compute document's static quality score (or static rank), which is often used to speed up search query evaluation.[ref]

</ref>
* Query-dependent or dynamic features — those features, which depend both on the contents of the document and the query, such as TF-IDF score or other non-machine-learned ranking functions.
* Query features, which depend only on the query. For example, the number of words in a query.
Some examples of features, which were used in the well-known LETOR dataset:LETOR 3.0. A Benchmark Collection for Learning to Rank for Information Retrieval
* TF, TF-IDF, BM25, and language modeling scores of document's zones (title, body, anchors text, URL) for a given query;
* Lengths and IDF sums of document's zones;
* Document's PageRank, HITS ranks and their variants.
Selecting and designing good features is an important area in machine learning, which is called feature engineering.

== Evaluation measures ==

There are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics.
Examples of ranking quality measures:
* Mean average precision (MAP);
* DCG and NDCG;
* Precision@n, NDCG@n, where "@n" denotes that the metrics are evaluated only on top n documents;
* Mean reciprocal rank;
* Kendall's tau
DCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used.http://www.stanford.edu/class/cs276/handouts/lecture15-learning-ranking.ppt Other metrics such as MAP, MRR and precision, are defined only for binary judgements.
Recently, there have been proposed several new evaluation metrics which claim to model user's satisfaction with search results better than the DCG metric:
* Expected reciprocal rank (ERR);
* Yandex's pfound.[ref]
 (in Russian)[/ref]
Both of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document.

== Approaches ==

Tie-Yan Liu of Microsoft Research Asia in his paper "Learning to Rank for Information Retrieval" and talks at several leading conferences has analyzed existing algorithms for learning to rank problems and categorized them into three groups by their input representation and loss function:

=== Pointwise approach ===

In this case it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then learning-to-rank problem can be approximated by a regression problem — given a single query-document pair, predict its score.
A number of existing supervised machine learning algorithms can be readily used for this purpose. Ordinal regression and classification algorithms can also be used in pointwise approach when they are used to predict score of a single query-document pair, and it takes a small, finite number of values.

=== Pairwise approach ===

In this case learning-to-rank problem is approximated by a classification problem — learning a binary classifier which can tell which document is better in a given pair of documents. The goal is to minimize average number of inversions in ranking.

=== Listwise approach ===

These algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to ranking model's parameters, and so continuous approximations or bounds on evaluation measures have to be used.

=== List of methods ===

A partial list of published learning-to-rank algorithms is shown below with years of first publication of each method:
:{|class="wikitable sortable"
! Year || Name || Type || Notes
|-
| 1989 || http://dx.doi.org/10.1145/65943.65944 OPRF || 2 pointwise || Polynomial regression (instead of machine learning, this work refers to pattern recognition, but the idea is the same)
|-
| 1992 || http://dx.doi.org/10.1145/133160.133199 SLR|| 2 pointwise || Staged logistic regression
|-
| 2000 || Ranking SVM (RankSVM) || 2 pairwise ||  A more recent exposition is in, which describes an application to ranking using clickthrough logs.
|-
| 2002 || Pranking || 1 pointwise || Ordinal regression.
|-
| 2003  || RankBoost || 2 pairwise ||
|-
| 2005 || RankNet || 2 pairwise ||
|-
| 2006 || IR-SVM || 2 pairwise || Ranking SVM with query-level normalization in the loss function.
|-
| 2006 || LambdaRank || 3 listwise || RankNet in which pairwise loss function is multiplied by the change in the IR metric caused by a swap.
|-
| 2007 || AdaRank || 3 listwise ||
|-
| 2007 || FRank || 2 pairwise || Based on RankNet, uses a different loss function - fidelity loss.
|-
| 2007 || GBRank || 2 pairwise || 
|-
| 2007 || ListNet || 3 listwise ||
|-
| 2007 || McRank || 1 pointwise ||
|-
| 2007 || QBRank || 2 pairwise ||
|-
| 2007 || RankCosine || 3 listwise ||
|-
| 2007 || RankGP || 3 listwise ||
|-
| 2007 || RankRLS || 2 pairwise ||
Regularized least-squares based ranking. The work is extended in
 to learning to rank from general preference graphs.
|-
| 2007 || SVMmap || 3 listwise ||
|-
| 2008 || LambdaMART || 3 listwise || Winning entry in the recent Yahoo Learning to Rank competition used an ensemble of LambdaMART models.C. Burges. (2010). From RankNet to LambdaRank to LambdaMART: An Overview.
|-
| 2008 || ListMLE || 3 listwise || Based on ListNet.
|-
| 2008 || PermuRank || 3 listwise ||
|-
| 2008 || SoftRank || 3 listwise ||
|-
| 2008 || Ranking RefinementRong Jin, Hamed Valizadegan, Hang Li, Ranking Refinement and Its Application for Information Retrieval, in International Conference on World Wide Web (WWW), 2008. || 2 pairwise || A semi-supervised approach to learning to rank that uses Boosting.
|-
| 2008 || SSRankBoostMassih-Reza Amini, Vinh Truong, Cyril Goutte, A Boosting Algorithm for Learning Bipartite Ranking Functions with Partially Labeled Data, International ACM SIGIR conference, 2008. The code is available for research purposes.  || 2 pairwise|| An extension of RankBoost to learn with partially labeled data (semi-supervised learning to rank)
|-
| 2008 || SortNetLeonardo Rigutini, Tiziano Papini, Marco Maggini, Franco Scarselli, "SortNet: learning to rank by a neural-based sorting algorithm", SIGIR 2008 workshop: Learning to Rank for Information Retrieval, 2008 || 2 pairwise|| SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. 
|-
| 2009 || MPBoost || 2 pairwise || Magnitude-preserving variant of RankBoost. The idea is that the more unequal are labels of a pair of documents, the harder should the algorithm try to rank them.
|-
| 2009 || BoltzRank || 3 listwise || Unlike earlier methods, BoltzRank produces a ranking model that looks during query time not just at a single document, but also at pairs of documents.
|-
| 2009 || BayesRank || 3 listwise || Based on ListNet.
|-
| 2010 || NDCG BoostHamed Valizadegan, Rong Jin, Ruofei Zhang, Jianchang Mao, Learning to Rank by Optimizing NDCG Measure, in Proceeding of Neural Information Processing Systems (NIPS), 2010. || 3 listwise || A boosting approach to optimize NDCG.
|-
| 2010 || GBlend || 2 pairwise || Extends GBRank to the learning-to-blend problem of jointly solving multiple learning-to-rank problems with some shared features.
|-
| 2010 || IntervalRank || 2 pairwise & listwise || 
|-
| 2010 || CRR || 2 pointwise & pairwise || Combined Regression and Ranking. Uses stochastic gradient descent to optimize a linear combination of a pointwise quadratic loss and a pairwise hinge loss from Ranking SVM.
|}
Note: as most supervised learning algorithms can be applied to pointwise case, only those methods which are specifically designed with ranking in mind are shown above.

== History ==

C. Manning et al.. Sections 7.4 and 15.5 trace earliest works on learning to rank problem to papers in late 1980s and early 1990s. They suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques.
In mid-1990s Berkeley researchers used logistic regression to train a successful ranking function at TREC conference.
Several conferences, such as NIPS, SIGIR and ICML had workshops devoted to the learning-to-rank problem since mid-2000s, and this has stimulated much of academic research.

=== Practical usage by search engines ===

Commercial web search engines began using machine learned ranking systems since 2000s. One of the first search engines to start using it was AltaVista (later its technology was acquired by Overture, and then Yahoo), which launched a gradient boosting-trained ranking function in April 2003.Jan O. Pedersen. The MLR Story
Bing's search is said to be powered by RankNet algorithm,Bing Search Blog: User Needs, Features and the Science behind Bing which was invented at Microsoft Research in 2005.
In November 2009 a Russian search engine Yandex announcedYandex corporate blog entry about new ranking model "Snezhinsk" (in Russian) that it had significantly increased its search quality due to deployment of a new proprietary MatrixNet algorithm, a variant of gradient boosting method which uses oblivious decision trees.The algorithm wasn't disclosed, but a few details were made public in http://download.yandex.ru/company/experience/GDD/Zadnie_algoritmy_Karpovich.pdf and http://download.yandex.ru/company/experience/searchconf/Searchconf_Algoritm_MatrixNet_Gulin.pdf. Recently they have also sponsored a machine-learned ranking competition "Internet Mathematics 2009"Yandex's Internet Mathematics 2009 competition page based on their own search engine's production data. Yahoo has announced a similar competition in 2010.Yahoo Learning to Rank Challenge
As of 2008, Google's Peter Norvig denied that their search engine exclusively relies on machine-learned ranking.[ref]
</ref> [[Cuil]]'s CEO, [[Tom Costello]], suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models "learn what people say they like, not what people actually like".<ref>
</ref>

== References ==

== External links ==

; Competitions and public datasets
* LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval
* Yandex's Internet Mathematics 2009
* Yahoo! Learning to Rank Challenge
* Microsoft Learning to Rank Datasets
; Open Source code
* Parallel C++/MPI implementation of Gradient Boosted Regression Trees for ranking, released September 2011
* C++ implementation of Gradient Boosted Regression Trees and Random Forests for ranking

